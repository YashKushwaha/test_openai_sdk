{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e2be3d-1d98-4081-8761-cd01ea54aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957791a9-fbb9-497c-b15e-6eaaa7ed2c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OpenAI in module openai:\n",
      "\n",
      "class OpenAI(openai._base_client.SyncAPIClient)\n",
      " |  OpenAI(\n",
      " |      *,\n",
      " |      api_key: 'str | None | Callable[[], str]' = None,\n",
      " |      organization: 'str | None' = None,\n",
      " |      project: 'str | None' = None,\n",
      " |      webhook_secret: 'str | None' = None,\n",
      " |      base_url: 'str | httpx.URL | None' = None,\n",
      " |      websocket_base_url: 'str | httpx.URL | None' = None,\n",
      " |      timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN,\n",
      " |      max_retries: 'int' = 2,\n",
      " |      default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      default_query: 'Mapping[str, object] | None' = None,\n",
      " |      http_client: 'httpx.Client | None' = None,\n",
      " |      _strict_response_validation: 'bool' = False\n",
      " |  ) -> 'None'\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      OpenAI\n",
      " |      openai._base_client.SyncAPIClient\n",
      " |      openai._base_client.BaseClient\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      *,\n",
      " |      api_key: 'str | None | Callable[[], str]' = None,\n",
      " |      organization: 'str | None' = None,\n",
      " |      project: 'str | None' = None,\n",
      " |      webhook_secret: 'str | None' = None,\n",
      " |      base_url: 'str | httpx.URL | None' = None,\n",
      " |      websocket_base_url: 'str | httpx.URL | None' = None,\n",
      " |      timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN,\n",
      " |      max_retries: 'int' = 2,\n",
      " |      default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      default_query: 'Mapping[str, object] | None' = None,\n",
      " |      http_client: 'httpx.Client | None' = None,\n",
      " |      _strict_response_validation: 'bool' = False\n",
      " |  ) -> 'None' from openai._client.OpenAI\n",
      " |      Construct a new synchronous OpenAI client instance.\n",
      " |\n",
      " |      This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n",
      " |      - `api_key` from `OPENAI_API_KEY`\n",
      " |      - `organization` from `OPENAI_ORG_ID`\n",
      " |      - `project` from `OPENAI_PROJECT_ID`\n",
      " |      - `webhook_secret` from `OPENAI_WEBHOOK_SECRET`\n",
      " |\n",
      " |  audio = <functools.cached_property object>\n",
      " |  batches = <functools.cached_property object>\n",
      " |  beta = <functools.cached_property object>\n",
      " |  chat = <functools.cached_property object>\n",
      " |  completions = <functools.cached_property object>\n",
      " |  containers = <functools.cached_property object>\n",
      " |  conversations = <functools.cached_property object>\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      api_key: 'str | Callable[[], str] | None' = None,\n",
      " |      organization: 'str | None' = None,\n",
      " |      project: 'str | None' = None,\n",
      " |      webhook_secret: 'str | None' = None,\n",
      " |      websocket_base_url: 'str | httpx.URL | None' = None,\n",
      " |      base_url: 'str | httpx.URL | None' = None,\n",
      " |      timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN,\n",
      " |      http_client: 'httpx.Client | None' = None,\n",
      " |      max_retries: 'int | NotGiven' = NOT_GIVEN,\n",
      " |      default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      set_default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      default_query: 'Mapping[str, object] | None' = None,\n",
      " |      set_default_query: 'Mapping[str, object] | None' = None,\n",
      " |      _extra_kwargs: 'Mapping[str, Any]' = {}\n",
      " |  ) -> 'Self' from openai._client.OpenAI\n",
      " |      Create a new client instance re-using the same options given to the current client with optional overriding.\n",
      " |\n",
      " |  embeddings = <functools.cached_property object>\n",
      " |  evals = <functools.cached_property object>\n",
      " |  files = <functools.cached_property object>\n",
      " |  fine_tuning = <functools.cached_property object>\n",
      " |  images = <functools.cached_property object>\n",
      " |  models = <functools.cached_property object>\n",
      " |  moderations = <functools.cached_property object>\n",
      " |  realtime = <functools.cached_property object>\n",
      " |  responses = <functools.cached_property object>\n",
      " |  uploads = <functools.cached_property object>\n",
      " |  vector_stores = <functools.cached_property object>\n",
      " |  videos = <functools.cached_property object>\n",
      " |  webhooks = <functools.cached_property object>\n",
      " |  with_options = copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      api_key: 'str | Callable[[], str] | None' = None,\n",
      " |      organization: 'str | None' = None,\n",
      " |      project: 'str | None' = None,\n",
      " |      webhook_secret: 'str | None' = None,\n",
      " |      websocket_base_url: 'str | httpx.URL | None' = None,\n",
      " |      base_url: 'str | httpx.URL | None' = None,\n",
      " |      timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN,\n",
      " |      http_client: 'httpx.Client | None' = None,\n",
      " |      max_retries: 'int | NotGiven' = NOT_GIVEN,\n",
      " |      default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      set_default_headers: 'Mapping[str, str] | None' = None,\n",
      " |      default_query: 'Mapping[str, object] | None' = None,\n",
      " |      set_default_query: 'Mapping[str, object] | None' = None,\n",
      " |      _extra_kwargs: 'Mapping[str, Any]' = {}\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  with_raw_response = <functools.cached_property object>\n",
      " |  with_streaming_response = <functools.cached_property object>\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  auth_headers\n",
      " |\n",
      " |  default_headers\n",
      " |\n",
      " |  qs\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'api_key': 'str', 'organization': 'str | None', 'pr...\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._base_client.SyncAPIClient:\n",
      " |\n",
      " |  __enter__(self: '_T') -> '_T'\n",
      " |\n",
      " |  __exit__(\n",
      " |      self,\n",
      " |      exc_type: 'type[BaseException] | None',\n",
      " |      exc: 'BaseException | None',\n",
      " |      exc_tb: 'TracebackType | None'\n",
      " |  ) -> 'None'\n",
      " |\n",
      " |  close(self) -> 'None'\n",
      " |      Close the underlying HTTPX client.\n",
      " |\n",
      " |      The client will *not* be usable after this.\n",
      " |\n",
      " |  delete(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      body: 'Body | None' = None,\n",
      " |      options: 'RequestOptions' = {}\n",
      " |  ) -> 'ResponseT'\n",
      " |\n",
      " |  get(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      options: 'RequestOptions' = {},\n",
      " |      stream: 'bool' = False,\n",
      " |      stream_cls: 'type[_StreamT] | None' = None\n",
      " |  ) -> 'ResponseT | _StreamT'\n",
      " |\n",
      " |  get_api_list(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      model: 'Type[object]',\n",
      " |      page: 'Type[SyncPageT]',\n",
      " |      body: 'Body | None' = None,\n",
      " |      options: 'RequestOptions' = {},\n",
      " |      method: 'str' = 'get'\n",
      " |  ) -> 'SyncPageT'\n",
      " |\n",
      " |  is_closed(self) -> 'bool'\n",
      " |\n",
      " |  patch(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      body: 'Body | None' = None,\n",
      " |      options: 'RequestOptions' = {}\n",
      " |  ) -> 'ResponseT'\n",
      " |\n",
      " |  post(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      body: 'Body | None' = None,\n",
      " |      options: 'RequestOptions' = {},\n",
      " |      files: 'RequestFiles | None' = None,\n",
      " |      stream: 'bool' = False,\n",
      " |      stream_cls: 'type[_StreamT] | None' = None\n",
      " |  ) -> 'ResponseT | _StreamT'\n",
      " |\n",
      " |  put(\n",
      " |      self,\n",
      " |      path: 'str',\n",
      " |      *,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      body: 'Body | None' = None,\n",
      " |      files: 'RequestFiles | None' = None,\n",
      " |      options: 'RequestOptions' = {}\n",
      " |  ) -> 'ResponseT'\n",
      " |\n",
      " |  request(\n",
      " |      self,\n",
      " |      cast_to: 'Type[ResponseT]',\n",
      " |      options: 'FinalRequestOptions',\n",
      " |      *,\n",
      " |      stream: 'bool' = False,\n",
      " |      stream_cls: 'type[_StreamT] | None' = None\n",
      " |  ) -> 'ResponseT | _StreamT'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from openai._base_client.SyncAPIClient:\n",
      " |\n",
      " |  __orig_bases__ = (openai._base_client.BaseClient[httpx.Client, openai....\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._base_client.BaseClient:\n",
      " |\n",
      " |  platform_headers(self) -> 'Dict[str, str]'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from openai._base_client.BaseClient:\n",
      " |\n",
      " |  custom_auth\n",
      " |\n",
      " |  default_query\n",
      " |\n",
      " |  user_agent\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openai._base_client.BaseClient:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  base_url\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bb7b84-d130-443d-a16f-cbd6b1ddc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://localhost:11434/api/generate'\n",
    "api_key = 'ollama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae904a0-4c55-483d-8972-dea9621daca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url = base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece304b8-7f60-4d42-9ef1-71f0a971b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bf9e51-dc6a-4f2f-a910-181a057faa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'qwen3:14b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01467e5-51f3-4103-9e7d-3f3673e5c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2bb4d4-bf09-4bb2-9d07-df020bd76ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Say this is a test',\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516b4056-317d-4275-8e97-f91a61a1de55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-452',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': 'Understood, this is a test. How can I assist you today?',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None,\n",
       "    'reasoning': 'Okay, the user said \"Say this is a test.\" Hmm, I need to figure out what they\\'re asking for. Maybe they want me to confirm that this is a test scenario? Or perhaps they\\'re setting up a test and need a response.\\n\\nFirst, I should check if there\\'s any context I\\'m missing. The user hasn\\'t provided much information. They might be testing my ability to follow instructions or check if I can recognize when a conversation is part of a test. \\n\\nI should respond in a way that acknowledges their statement and offers further assistance. Maybe something like, \"Understood, this is a test. How can I assist you today?\" That way, I confirm it\\'s a test and invite them to ask questions. \\n\\nWait, but the user might be expecting a specific response. If this is a test for the model\\'s compliance or accuracy, I need to make sure I don\\'t provide any incorrect information. Since the instruction is straightforward, just confirming it\\'s a test and asking how I can help should be appropriate. \\n\\nAlso, I should avoid overcomplicating the response. The user might want a simple confirmation. Let me go with that approach.\\n'}}],\n",
       " 'created': 1764765807,\n",
       " 'model': 'qwen3:14b',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_ollama',\n",
       " 'usage': {'completion_tokens': 258,\n",
       "  'prompt_tokens': 15,\n",
       "  'total_tokens': 273,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df12275c-7160-4f0d-b713-07590a76a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_completion = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097f6a81-6f9f-4fd2-8642-468076b46c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'qwen3:14b',\n",
       "   'created': 1764683523,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'library'},\n",
       "  {'id': 'gemma3:12b',\n",
       "   'created': 1764681531,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'library'},\n",
       "  {'id': 'qwen3-vl:8b',\n",
       "   'created': 1764675308,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'library'}],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_completion.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5929a3f-fa97-408a-a3d5-35686a8e4e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26fd35fb-cb41-4d45-9c8d-2b88244504d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friends=[FriendInfo(name='Ollama', age=22, is_available=False), FriendInfo(name='Alonso', age=23, is_available=True)]\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# Define the schema for the response\n",
    "class FriendInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    is_available: bool\n",
    "\n",
    "class FriendList(BaseModel):\n",
    "    friends: list[FriendInfo]\n",
    "\n",
    "try:\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        temperature=0,\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format\"}\n",
    "        ],\n",
    "        response_format=FriendList,\n",
    "    )\n",
    "\n",
    "    friends_response = completion.choices[0].message\n",
    "    if friends_response.parsed:\n",
    "        print(friends_response.parsed)\n",
    "    elif friends_response.refusal:\n",
    "        print(friends_response.refusal)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43cd47d9-1ed8-455f-b0a4-6b183fd811cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FriendList(friends=[FriendInfo(name='Ollama', age=22, is_available=False), FriendInfo(name='Alonso', age=23, is_available=True)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_response.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18aa7d1f-6659-4e90-be97-de1ccb112786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '{\\n  \"friends\": [\\n    {\\n      \"name\": \"Ollama\",\\n      \"age\": 22,\\n      \"is_available\": false\\n    },\\n    {\\n      \"name\": \"Alonso\",\\n      \"age\": 23,\\n      \"is_available\": true\\n    }\\n  ]\\n}',\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'annotations': None,\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None,\n",
       " 'parsed': {'friends': [{'name': 'Ollama', 'age': 22, 'is_available': False},\n",
       "   {'name': 'Alonso', 'age': 23, 'is_available': True}]},\n",
       " 'reasoning': 'Okay, the user wants a list of their two friends in JSON format. Let me start by understanding the details they provided.\\n\\nFirst friend is Ollama, 22 years old, and busy saving the world. Second is Alonso, 23, wants to hang out. Need to structure this into a JSON array with objects for each friend.\\n\\nEach object should have name, age, and a description. For Ollama, the description is \"busy saving the world\". Alonso\\'s is \"wants to hang out\". \\n\\nCheck if the JSON syntax is correct: keys in quotes, commas between objects, proper brackets. Also, ensure the age is a number, not a string. Let me put that together.\\n'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fdea1-931f-4ddc-b5b9-5f1c1e21a32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
